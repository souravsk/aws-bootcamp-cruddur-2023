# Week 2 — Distributed Tracing

# Distributed Tracing

Distributed tracing is a method of tracking application requests as they flow from frontend devices to backend services and databases. Developers can use distributed tracing to troubleshoot requests that exhibit high latency or errors.

# Honeycomb

[Honeycomb](https://www.honeycomb.io/why-honeycomb/) is a fast analysis tool that reveals how your code is experienced in complex and unpredictable environments. Troubleshoot complex relationships within your distributed services and solve problems faster.

Adding the Honeycomb API Key 

![](img/week-2/week-2%20(1).png)

Installing the opentelemetry-api, SDK, etc the is requrited to run the homeycomb

![](img/week-2/week-2%20(2).png)

Importing the files

![](img/week-2/week-2%20(3).png)

Now running the docker-compose file get get data on the honeycomb website

![](img/week-2/week-2%20(4).png)

Let visit honeycomb website and see if it is collecting the log or not.

![](img/week-2/week-2%20(5).png)

![](img/week-2/week-2%20(6).png)

![](img/week-2/week-2%20(7).png)

![](img/week-2/week-2%20(8).png)

![](img/week-2/week-2%20(9).png)

Now let's create query and see the results

![](img/week-2/week-2%20(10).png)

![](img/week-2/week-2%20(11).png)

![](img/week-2/week-2%20(12).png)
# Logs

Log trends are the peak of log analysis and a key part of observability.

Log trends use aggregated log data to help you identify bottlenecks, performance problems, or even cyber threats, based on data gathered over time

## Current State of Logging

ON-Premise Logs

- Infrastructure
- Application
- Anti-virus
- Firewall
- etc

Cloud Logs

- Infrastructures
- Application
- Anti-virus
- Firewall
- etc

So whether it is on-premise or in the cloud it gets logs but it is a little different in the case of the cloud for Infrastructures and Applications because things work differently on the cloud.

## Why it is stressful to analyse logs

- Time-Consuming
- Tons of data with no context for why the security events?
- Finding an error is like finding a needle in a haystack.
- This problem is in everywhere whether it is monolith or services or microservices

# Observability

observability is the extent to which you can understand the internal state or condition of a complex system based only on knowledge of its external outputs

- it Decreased alert fatigue for security operation Teams
- Visibility of end-to-end logs, metrics and tracking
- Troubleshoot and resolve things quickly without costing too much money.
- Understand applications health
- Accelerate Collaboration between teams
- Reduce overall operational cost
- Increase customer sanctification.

## 3 Pillars of observability

1. Metrics
2. Traces
3. Logs

# **Monitoring and observability**

**Monitoring** is tooling or a technical solution that allows teams to watch and understand the state of their systems. Monitoring is based on gathering predefined sets of metrics or logs. 

**Observability** is tooling or a technical solution that allows teams to actively debug their system. Observability is based on exploring properties and patterns not defined in advance.

# what is XRay

AWS X-Ray is a service that collects data about requests that your application serves and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases, and web APIs.

![](img/week-2/week-2%20(30).png)

## **AWS X-Ray API**

The X-Ray API provides access to all X-Ray functionality through the AWS SDK, AWS Command Line Interface, or directly over HTTPS. The [X-Ray API Reference](https://docs.aws.amazon.com//xray/latest/api/Welcome.html) documents input parameters for each API action, and the fields and data types that they return.

## **AWS X-Ray daemon**

The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service.

## **Service map**

The service map is a visual representation of the trace data that's generated by your applications. The map shows service nodes that serve requests, upstream client nodes that represent the origins of the requests and downstream service nodes that represent web services and resources that are used by an application while processing a request.

## **Traces**

**Traces** page in the X-Ray console to find traces by URL, response code, or other data from the trace summary. After selecting a trace from the trace list, the **Trace details** page displays a map of service nodes involved in the selected trace, along with a timeline of trace segments.

Let try some example

First, we will add the `xray.json` file which has name, serviceName, etc.

![](img/week-2/week-2%20(13).png)

Now Import the file in `app.py`

![](img/week-2/week-2%20(14).png)

Now with the help of AWS CLI, we will run the application and create an X-ray

![](img/week-2/week-2%20(15).png)

![](img/week-2/week-2%20(16).png)

Now let's check that all the ports are running...

![](img/week-2/week-2%20(17).png)

Looks like the Xray-daemon is not started let's check the logs of the X-ray

![](img/week-2/week-2%20(18).png)

As I can see it is working.

Now let's go to the AWS X-Ray to check if is it collecting the data or not

![](img/week-2/week-2%20(19).png)

![](img/week-2/week-2%20(20).png)

Yes!!! it does collecting

# Cloudwatch Logs
 Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.

CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. CloudWatch Logs enables you to see all of your logs, regardless of their source, as a single and consistent flow of events ordered by time, and you can query them and sort them based on other dimensions, group them by specific fields, create custom computations with a powerful query language, and visualize log data in dashboards.

Let's see some examples

First import the Logger the will collect the data and give it to the AWS

![](img/week-2/week-2%20(28).png)

Now run the docker-compose again and visit the backend server at the home section so that it can generate some data.

Before going to the AWS website let's first look into the X-ray docker container logs to see the is it sending the data or not.

![](img/week-2/week-2%20(25).png)

Now run the docker-compose again and let's go to the aws Cloudwatch and click on the log group to see all the logs that it has collected.

![](img/week-2/week-2%20(26).png)

it looks like we got some data let's look into the latest one.

![](img/week-2/week-2%20(27).png)

Yes!! it is our data log.

# what is Instrumenting

Instrumenting your application involves sending trace data for incoming and outbound requests and other events within your application, along with metadata about each request. There are several different instrumentation options you can choose from or combine, based on your particular requirements:

- *Auto instrumentation* – instrument your application with zero code changes, typically via configuration changes, adding an auto-instrumentation agent, or other mechanisms.
- *Library instrumentation* – make minimal application code changes to add pre-built instrumentation targeting specific libraries or frameworks, such as the AWS SDK, Apache HTTP clients, or SQL clients.
- *Manual instrumentation* – add an instrumentation code to your application at each location where you want to send trace information.

# Rollbar

Rollbar is a cloud-based bug-tracking and monitoring solution that caters to organizations of all sizes. Rollbar supports multiple programming languages and frameworks like JavaScript, Python, .NET, Drupal, WordPress and Pyramid. The solution can also be deployed on-premises at the user end.

Rollbar provides automatic error grouping based on an error's root cause and also gives users an option to customize these grouping rules. Notification filters help users to set and prioritize notifications by error type. The solution also offers workflow control capabilities that allow users to tag issues as active, resolved or muted.

Rollbar enables users to track their deployments, provide information in the form of graphs, manage timelines and maintain deployment history. The solution adheres to industry standards and provides 2048-bit end-to-end SSL encryption, two-factor authentication, login controls and audit logs. Rollbar comes with Rollbar Query Language that allows users to communicate with the database.

Let's try some example

First, create an account on the rollbar website then we will import some required files that are needed. We will then create a new route that will return as `hello world`. So after writing the code and running the docker-compose command let's visit the route that is created for this example which is `rollbar/test`

![](img/week-2/week-2%20(29).png)

Nice!!! it gives a `hello world` that means it is working.

Let's go to the rollbar website to see if it did have the logs data that is generated.

![](img/week-2/week-2%20(31).png)

Yes, it did.